(window.webpackJsonp=window.webpackJsonp||[]).push([[552],{708:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return i})),n.d(t,"metadata",(function(){return o})),n.d(t,"rightToc",(function(){return r})),n.d(t,"default",(function(){return c}));var a=n(1),s=(n(0),n(1117));const i={last_modified_on:"2020-04-10",id:"how-we-test-vector",title:"How We Test Vector",description:"A survey of techniques we've found useful",author_github:"https://github.com/lukesteensen",tags:["type: post","domain: testing"]},o={permalink:"/blog/how-we-test-vector",source:"@site/blog/2020-04-09-how-we-test-vector.md",description:"A survey of techniques we've found useful",date:"2020-04-09T00:00:00.000Z",tags:[{label:"type: post",permalink:"/blog/tags/type-post"},{label:"domain: testing",permalink:"/blog/tags/domain-testing"}],title:"How We Test Vector",readingTime:14.8,truncated:!0,nextItem:{title:"Introducing Vector",permalink:"/blog/introducing-vector"}},r=[{value:"Example-based testing",id:"example-based-testing",children:[{value:"Unit tests",id:"unit-tests",children:[]},{value:"Integration tests",id:"integration-tests",children:[]}]},{value:"Generative testing",id:"generative-testing",children:[{value:"Property-based tests",id:"property-based-tests",children:[]},{value:"Model-based testing",id:"model-based-testing",children:[]},{value:"Fuzz testing",id:"fuzz-testing",children:[]}]},{value:"Black-box testing",id:"black-box-testing",children:[{value:"Performance tests",id:"performance-tests",children:[]},{value:"Correctness tests",id:"correctness-tests",children:[]},{value:"Reliability tests",id:"reliability-tests",children:[]}]},{value:"Conclusion",id:"conclusion",children:[]}],l={rightToc:r};function c({components:e,...t}){return Object(s.b)("wrapper",Object(a.a)({},l,t,{components:e,mdxType:"MDXLayout"}),Object(s.b)("p",null,"When we set out to build Vector, we knew that reliability and performance were\ntwo of our top priorities. We also knew that even the best of intentions would\nnot be enough to make certain those qualities were realized and reflected in our\nusers' production deployments. Since then, we've been continuously evolving and\nexpanding our approach to achieving that level of quality."),Object(s.b)("p",null,"There are a few factors that make ensuring robustness a particularly difficult\ntask for software like Vector:"),Object(s.b)("ol",null,Object(s.b)("li",{parentName:"ol"},'It\'s relatively new and not as "battle tested" as more widely deployed\nsoftware.'),Object(s.b)("li",{parentName:"ol"},"The vast majority of its functionality lives at the edges, interfacing with\nvarious external systems."),Object(s.b)("li",{parentName:"ol"},"Instead of a monolithic application designed for a single task, it is\na collection of components that can be assembled into a near-infinite number\nof configurations.")),Object(s.b)("p",null,"This challenge has given us a unique opportunity to apply wide variety of\ntesting techniques:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Example-based testing",Object(s.b)("ul",{parentName:"li"},Object(s.b)("li",{parentName:"ul"},"Unit tests"),Object(s.b)("li",{parentName:"ul"},"Integration tests"))),Object(s.b)("li",{parentName:"ul"},"Generative testing",Object(s.b)("ul",{parentName:"li"},Object(s.b)("li",{parentName:"ul"},"Property-based testing"),Object(s.b)("li",{parentName:"ul"},"Model-based testing"),Object(s.b)("li",{parentName:"ul"},"Fuzz testing"))),Object(s.b)("li",{parentName:"ul"},"Black-box testing",Object(s.b)("ul",{parentName:"li"},Object(s.b)("li",{parentName:"ul"},"Performance tests"),Object(s.b)("li",{parentName:"ul"},"Correctness tests"),Object(s.b)("li",{parentName:"ul"},"Reliability tests")))),Object(s.b)("p",null,"While there's no one perfect solution to overcome these difficulties, we've\nfound that the combination of these techniques at different layers of the stack\nhas given us a good level of confidence in Vector's behavior."),Object(s.b)("p",null,"In this post, we'll discuss briefly how we're using each of these types of\ntests, their strengths and weaknesses, as well as any tips we have for using\nthem effectively."),Object(s.b)("h2",{id:"example-based-testing"},"Example-based testing"),Object(s.b)("p",null,"We'll start off with the types of tests you're likely most familiar with, the\nhumble unit and integration tests. These are the bread and butter of almost\nevery test suite, and for good reason."),Object(s.b)("p",null,'We group them into "example-based" because they both follow the same general\npattern. As the developer, you come up with an example input, you have your code\nprocess that example, and then you assert that the outcome is what you expected.'),Object(s.b)("p",null,"A great deal has been written on these types of tests already, so we'll try to\nkeep this section brief and focused on situations where these techniques start\nto break down."),Object(s.b)("h3",{id:"unit-tests"},"Unit tests"),Object(s.b)("p",null,"A unit test is generally defined by the idea of isolation. This can mean\ndifferent things to different people, but a common definition is that the test\nis both isolated from the outside world (i.e. no network calls, reading files,\netc) and exercises a single component of the system (e.g. one function, class,\netc)."),Object(s.b)("p",null,"In Vector, we've found unit tests to be a particularly great fit for our\ntransforms. This makes sense, since transforms are effectively isolated\nfunctions that take events in and return events out. For the same reason, we\ntend to use unit tests extensively around the encoder portions of our sinks."),Object(s.b)("p",null,"For other types of components, so much of the functionality is focused on\ncommunication with external systems that it can be difficult to isolate logic\nenough for a unit test. As much as possible, we try to critically assess that\ndifficulty and use it to help find ways we can refactor the design to be more\nmodular and therefore amenable to unit testing. Unit tests are an excellent\nsource of design feedback, so we want to feel that pain and refactor when\nthey're difficult to write."),Object(s.b)("p",null,"That being said, there are two specific places we often run into the limitations\nof unit tests. The first and more obvious is around pieces of code that are\nfundamentally not isolated. The second situation has to do with the size of the\ninput space and number of potential paths through the component under test. As\nan example-based testing strategy, the effectiveness of unit tests comes down to\nthe developer's ability to provide a thorough set of example inputs. This\nbecomes exponentially more difficult with each logical branch, and requires\nrecognizing paths you weren't considering when you initially wrote the code."),Object(s.b)("p",null,"Takeaways:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Isolation makes unit tests simple, fast, and reliable."),Object(s.b)("li",{parentName:"ul"},"If something is difficult to unit test, refactor until it's easy."),Object(s.b)("li",{parentName:"ul"},"As a human, be wary of your ability to think up exhaustive example inputs.")),Object(s.b)("h3",{id:"integration-tests"},"Integration tests"),Object(s.b)("p",null,"The category of integration tests is a bit of a catch-all. Roughly defined,\nthey're example-based tests that are explicitly not isolated and focus on the\ninteraction between two or more components."),Object(s.b)("p",null,"Given Vector's focus on integrating with a wide variety of external systems, we\nhave a higher ratio of integration tests than your average system. Even once\nwe've done all we can to isolate logic into small, unit-testable functions, we\nstill need to ensure the component as a whole does what it's supposed to. If\nunit tests tell you that something works in theory, integration tests tell you\nthat it ",Object(s.b)("em",{parentName:"p"},"should")," work in practice."),Object(s.b)("p",null,"Similar to unit tests, there are two major downsides to integration tests. The\nfirst is the same problem unit tests have with exhaustiveness, but intensely\nmagnified. Because they often cover the interaction of multiple whole systems,\nintegration tests will virtually never cover the combinatorial explosion of\npotential execution paths. Second, integration tests are often simply a pain to\nwrite and maintain. Given their dependence on external systems, they can be\ntedious to write, slow to run, and often flaky if their environment isn't set up\njust so."),Object(s.b)("p",null,"Takeaways:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"While they can be a pain, integration tests are a valuable sanity check that\nyour system will actually work for users."),Object(s.b)("li",{parentName:"ul"},"Don't overuse them trying to cover every possible scenario, because you can't.")),Object(s.b)("h2",{id:"generative-testing"},"Generative testing"),Object(s.b)("p",null,"The idea of generative testing is a direct response to the shared shortcoming of\nexample-based strategies like unit and integration testing. Humans are bad at\nvisualizing very large state spaces, and the state space of your code's possible\ninputs and execution paths is enormous. You also bring all the same biases when\nwriting tests as you did when writing the original code."),Object(s.b)("p",null,"Generative tests address these issues by taking the human out of the equation\nand having the computer generate millions of example inputs. The tradeoff is\nthat since you aren't simply hardcoding a list of inputs and their expected\noutputs anymore, you're forced to come up with more creative ways of identifying\nfailures."),Object(s.b)("h3",{id:"property-based-tests"},"Property-based tests"),Object(s.b)("p",null,"A simple way to think about property-based tests is that they are unit tests\nwhere the computer comes up with random example inputs. Since the test author\ncan no longer provide the expected output along with those example inputs, they\ninstead declare certain ",Object(s.b)("em",{parentName:"p"},"properties")," that must hold true across any combination\nof input and output. A classic example is testing a function that reverses\na list against the property that any list reversed twice must be equal to\nitself."),Object(s.b)("p",null,"While this is something you could do with no tooling support at all, there are\nsome pretty advanced libraries that make things easier (e.g. the venerable\n",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"http://www.cse.chalmers.se/~rjmh/QuickCheck/manual.html"}),"quickcheck")," and more recent ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://hypothesis.works/articles/what-is-property-based-testing/"}),"hypothesis"),"). Beyond simple random input\ngeneration, these tools often include valuable features like customizable\ngenerators and shrinking, which attempts to simplify failing inputs as much as\npossible before presenting them to you."),Object(s.b)("p",null,"In Vector, we use property-based tests to exercise our internal data\nserialization logic. We want to make sure that arbitrary input events can go\nthrough the full serialization and deserialization process without losing any\ninformation along the way. We use the Rust ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/AltSysrq/proptest"}),"proptest")," library to generate\ninput events and then test that the output event is equal to the input after\na full round trip through the system. Since this is an isolated, deterministic\nfunction, we can quickly and easily run millions of iterations of this test."),Object(s.b)("p",null,'While property-based tests are certainly better at covering the input space than\nunit tests, simple random generation of possible inputs can become a limitation.\nWithout some sense of what makes an input "interesting", property-based tests\nhave no way of intelligently exploring the space. In some cases, this can\nmeaning burning a lot of CPU without necessarily finding any new failures.'),Object(s.b)("p",null,"Takeaways:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Property-based tests help uncover more edge cases in your system's logic."),Object(s.b)("li",{parentName:"ul"},"Like unit tests, they're most effective when applied to isolated components."),Object(s.b)("li",{parentName:"ul"},'They can\'t directly test "correctness", only that your given set of invariants\nis upheld.')),Object(s.b)("h3",{id:"model-based-testing"},"Model-based testing"),Object(s.b)("p",null,"One particularly interesting application of property-based test tooling is\nsomething we've seen called ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://medium.com/@tylerneely/reliable-systems-series-model-based-property-testing-e89a433b360"}),"model-based testing"),". The basic idea is that you\nimplement a simplified model of your system (e.g. a hashmap is a simple model of\na key-value store), and then assert that for all possible inputs, your system\nshould produce the same output as the model."),Object(s.b)("p",null,"Vector inherited some of these tests from ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/postmates/cernan"}),"cernan"),", where its file tailing\nmodule originated (thanks ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/blt"}),"Brian"),"!). It works by generating random sequences\nof file writes, reads, rotations, truncations, etc, and applying them to both\na simple in-memory model of a filesystem as well as the actual file system being\ntailed by our file watcher implementation. It then verifies that the lines\nreturned by our watcher are the same as those returned from the simplified\nsimulation."),Object(s.b)("p",null,"In this strategy, the model is acting as an oracle and the quality of the test\ndepends on that oracle actually behaving correctly. That makes it a good match\nfor components with a relatively simple API but deeper inner complexity due to\nperformance optimizations, persistence, etc. Like normal property-based tests,\nthey may have trouble efficiently exploring the state space of especially\ncomplex components."),Object(s.b)("p",null,"Takeaways:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Model-based tests are a good match for components with deep implementations\nbut relatively shallow APIs."),Object(s.b)("li",{parentName:"ul"},'They rely on a model implementation simple enough to be "obviously correct",\nwhich is not possible for all systems.')),Object(s.b)("h3",{id:"fuzz-testing"},"Fuzz testing"),Object(s.b)("p",null,"At its most simplistic, fuzz testing is just feeding your program random data\nand seeing if it breaks. In that sense, you can think of it as a kind of\nexternal property-based testing, where the property is that your system should\nnot crash. This might not sound terribly interesting on its own, but modern\ntools (e.g. ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"http://lcamtuf.coredump.cx/afl/"}),"american fuzzy lop"),") have developed a superpower that gives them\na massive advantage over traditional property-based testing: using code-coverage\ninformation to guide input generation."),Object(s.b)("p",null,"With this critical feedback loop in place, tools can see when a particular input\nled to a new execution path. They can then intelligently evolve these\ninteresting inputs to prioritize finding even more new paths, zeroing in far\nmore efficiently on potentially unhandled edge cases."),Object(s.b)("p",null,'This is a particularly powerful technique for testing parsers. Where normal\nproperty-based tests might repeatedly attempt to parse random strings and never\nhappen upon anything remotely valid, a fuzz testing tool can gradually "learn"\nthe format being parsed and spend far more time exploring productive areas of\nthe input space.'),Object(s.b)("p",null,"Many of the parsers we use in Vector are prebuilt for various data formats and\nhave seen some fuzz testing in their upstream library. We did, however, write\nour ",Object(s.b)("inlineCode",{parentName:"p"},"tokenizer")," parser from scratch and it's unique in that it's not for\na specific format. Instead, it gives a best-effort attempt at breaking the input\nup into logical fields. We've found it to be a great fit for fuzz testing\nbecause the way that it handles strange and misshappen inputs is less important\nthan that fact that it will not panic and crash the program."),Object(s.b)("p",null,"One of the limitations of AFL-style fuzzing is the focus on random byte strings\nas inputs. This matches up really well with parsers, but maybe not that many\nother components in your system. The idea of ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/google/fuzzing/blob/master/docs/structure-aware-fuzzing.md"}),"structure-aware")," fuzzers looks\nto address this. One such tool is ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/loiclec/fuzzcheck-rs"}),"fuzzcheck"),", which we've been starting to\nexplore. Instead of byte strings, it works directly with the actual types of\nyour program. It also runs in-process with your system, making it simpler to\ndetect not just panics but also things like simple test failures. In many ways,\nit has the potential to combine the best of both fuzz testing and property-based\ntesting."),Object(s.b)("p",null,"Takeaways:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Feedback loops allow fuzz testing to efficiently explore extremely large input\nspaces, like those of a parser."),Object(s.b)("li",{parentName:"ul"},"Tools are advancing rapidly, making fuzz tests more convenient for more types\nof situations.")),Object(s.b)("h2",{id:"black-box-testing"},"Black-box testing"),Object(s.b)("p",null,"Even if all of the above testing strategies worked flawlessly and got us to 100%\nbranch coverage, we still wouldn't know for certain that Vector was performing\nat the level we expect. To answer that question, we need to run it as users run\nit and observe things like throughput, memory usage, CPU usage, etc."),Object(s.b)("p",null,"This is where the ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/timberio/vector-test-harness/"}),Object(s.b)("inlineCode",{parentName:"a"},"vector-test-harness"))," comes in. These are high-level,\nblack-box tests where we run various Vector configurations on deployed hardware,\ngenerating load and capturing metrics about its performance. And since they're\nblack-box tests (i.e. they require no access to or knowledge of Vector\ninternals), we can also provide configurations for similar tools to see how they\ncompare."),Object(s.b)("h3",{id:"performance-tests"},"Performance tests"),Object(s.b)("p",null,"The performance tests in our harness focus on generating as much load as the\ngiven configuration can handle and measuring throughput, memory use, etc. These\ntests capture our real-world performance in way that microbenchmarks can't, and\nthey give us a very useful point of comparison with other tools that may have\nmade different design decisions. If one of the metrics looks way off, that gives\nus a starting point to investigate why we're not performing as well as we think\nwe should."),Object(s.b)("p",null,"Since these tests are almost completely automated, we'll soon be looking to\nstart running them on a nightly basis and graphing the results over time. This\nshould give us an early warning signal in the case of a serious performance\nregression, and help us visualize our progress in making Vector faster and more\nefficient over time."),Object(s.b)("p",null,"Takeaways:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Behavior under load is an important part of the user experience and deserves\na significant testing investment."),Object(s.b)("li",{parentName:"ul"},"Regular, automated testing can generate valuable data for catching performance\nissues before they reach users.")),Object(s.b)("h3",{id:"correctness-tests"},"Correctness tests"),Object(s.b)("p",null,"Alongside those performance tests, we also have a set of tests we call\ncorrectness tests. The setup is quite similar to the performance tests, but the\nfocus is different. Instead of generating as much load as we can and watching\nthings like throughput and system resource use, we instead run each\nconfiguration through different interesting scenarios to see how they behave."),Object(s.b)("p",null,"For example, we have correctness tests around various flavors of file rotation,\ndisk persistence across restarts, nested JSON messages, etc. While these are\nbehaviors that we also test at various lower levels (e.g. unit and integration\ntests), covering a handful of important cases at this level of abstraction gives\nus some extra confidence that we are seeing exactly what our users will see."),Object(s.b)("p",null,"The ability to compare behaviors across competing tools is another bonus. Going\nthrough the process of setting up those tests gets us valuable experience\nworking with those other tools. We can see what works well in their\nconfiguration, documentation, etc, and identify areas where we can improve\nVector as a result."),Object(s.b)("p",null,"Takeaways:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},'Taking time to "zoom out" and test your system as a user would can help\nuncover blind spots and sanity-check behavior.'),Object(s.b)("li",{parentName:"ul"},"Evaluating similar tools can help build a better understanding of user\nexpectations.")),Object(s.b)("h3",{id:"reliability-tests"},"Reliability tests"),Object(s.b)("p",null,"A third category that we're currently working to integrate into\n",Object(s.b)("inlineCode",{parentName:"p"},"vector-test-harness")," is something we're calling reliability tests. These are\nsimilar to performance and correctness tests, except that they're designed to\nrun continuously and flush out errors that may occur only in rare environmental\ncircumstances."),Object(s.b)("p",null,"In a way, they're like simple, integration-level fuzz tests where changes in the\nenvironment over time provide input randomness. For example, running a week-long\nreliability test of our S3 sink exposed a bug where a specific kind of network\nfailure could lead to duplicate data when the retried request crossed\na timestamp boundary. That is not the type of failure we expect to be able to\ninduce in a local integration test, and the relevant factors (time and network\nconditions) were not those exercised by standard fuzzing or property-based\ntesting."),Object(s.b)("p",null,"The main challenge with these kinds of tests, aside from getting the requisite\nenvironment and harnesses up and running, is capturing sufficient context about\nthe environment at the time of the failure that you stand a chance at\nunderstanding and reproducing it. This task itself is a great test for our\ninternal observability, and any issue we can't reproduce is a sign that our\nlogging and metrics data needs to be improved."),Object(s.b)("p",null,"Another issue with these tests is that the vast majority of the time, nothing\nparticularly interesting is happening. Since we want to find bugs as quickly as\npossible, we can supplement the randomness of the environment by injecting\nvarious types of faults on our own. There are a variety of tools for this, such\nas ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/Shopify/toxiproxy"}),"Toxiproxy")," and ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://github.com/osrg/namazu"}),"Namazu"),"."),Object(s.b)("p",null,"Takeaways:"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"The environment is an important source of uncertainty in your system that is\ndifficult to simulate accurately."),Object(s.b)("li",{parentName:"ul"},"Observing bugs from a user's perspective incentivizes good internal\nobservability tooling")),Object(s.b)("h2",{id:"conclusion"},"Conclusion"),Object(s.b)("p",null,"Even with all of the above in place, we're continuously exploring ways to\nfurther increase our confidence in the reliability and performance of Vector.\nThat could mean anything from expanding our current test suites to be more\nthorough to adopting entirely new techniques to help cover more possible\nexecutions (e.g. ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://www.youtube.com/watch?v=4fFDFbi3toc"}),"simulation")," or ",Object(s.b)("a",Object(a.a)({parentName:"p"},{href:"https://www.hillelwayne.com/post/metamorphic-testing/"}),"metamorphic")," testing)."),Object(s.b)("p",null,"With some users running a Vector process on nearly every host in their\ninfrastructure, ensuring an extremely high level of robustness and efficiency is\nparamount. At the same time, those needs must be balanced with increasing\nVector's functional capabilities. Finding the right balance is an ongoing\nchallenge as the project grows and matures."))}c.isMDXComponent=!0}}]);